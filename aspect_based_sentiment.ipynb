{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Importing the training data\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')\n",
        "print(imdb_data.shape)\n",
        "print(imdb_data.head(10))\n",
        "\n",
        "# Summary of the dataset\n",
        "print(imdb_data.describe())\n",
        "\n",
        "# Sentiment count\n",
        "print(imdb_data['sentiment'].value_counts())\n",
        "\n",
        "# Split the dataset\n",
        "# Train dataset\n",
        "train_reviews = imdb_data.review[:40000]\n",
        "train_sentiments = imdb_data.sentiment[:40000]\n",
        "# Test dataset\n",
        "test_reviews = imdb_data.review[40000:]\n",
        "test_sentiments = imdb_data.sentiment[40000:]\n",
        "print(train_reviews.shape, train_sentiments.shape)\n",
        "print(test_reviews.shape, test_sentiments.shape)\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "# Setting English stopwords\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Removing the HTML strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'\\[[^]]*\\]', '', text)\n",
        "\n",
        "# Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "# Apply function on review column\n",
        "imdb_data['review'] = imdb_data['review'].apply(denoise_text)\n",
        "\n",
        "# Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "# Apply function on review column\n",
        "imdb_data['review'] = imdb_data['review'].apply(remove_special_characters)\n",
        "\n",
        "# Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps = PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# Apply function on review column\n",
        "imdb_data['review'] = imdb_data['review'].apply(simple_stemmer)\n",
        "\n",
        "# Removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "# Apply function on review column\n",
        "imdb_data['review'] = imdb_data['review'].apply(remove_stopwords)\n",
        "\n",
        "# Normalized train reviews\n",
        "norm_train_reviews = imdb_data.review[:40000]\n",
        "# Normalized test reviews\n",
        "norm_test_reviews = imdb_data.review[40000:]\n",
        "\n",
        "# Count vectorizer for bag of words\n",
        "cv = CountVectorizer(min_df=0, max_df=1, binary=False, ngram_range=(1, 3))\n",
        "# Transformed train reviews\n",
        "cv_train_reviews = cv.fit_transform(norm_train_reviews)\n",
        "# Transformed test reviews\n",
        "cv_test_reviews = cv.transform(norm_test_reviews)\n",
        "\n",
        "print('BOW_cv_train:', cv_train_reviews.shape)\n",
        "print('BOW_cv_test:', cv_test_reviews.shape)\n",
        "\n",
        "# Tfidf vectorizer\n",
        "tv = TfidfVectorizer(min_df=0, max_df=1, use_idf=True, ngram_range=(1, 3))\n",
        "# Transformed train reviews\n",
        "tv_train_reviews = tv.fit_transform(norm_train_reviews)\n",
        "# Transformed test reviews\n",
        "tv_test_reviews = tv.transform(norm_test_reviews)\n",
        "\n",
        "print('Tfidf_train:', tv_train_reviews.shape)\n",
        "print('Tfidf_test:', tv_test_reviews.shape)\n",
        "\n",
        "# Labeling the sentiment data\n",
        "lb = LabelBinarizer()\n",
        "# Transformed sentiment data\n",
        "sentiment_data = lb.fit_transform(imdb_data['sentiment'])\n",
        "print(sentiment_data.shape)\n",
        "\n",
        "# Splitting the sentiment data\n",
        "train_sentiments = sentiment_data[:40000]\n",
        "test_sentiments = sentiment_data[40000:]\n",
        "print(train_sentiments)\n",
        "print(test_sentiments)\n",
        "\n",
        "# Training the model\n",
        "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n",
        "# Fitting the model for Bag of Words\n",
        "lr_bow = lr.fit(cv_train_reviews, train_sentiments)\n",
        "print(lr_bow)\n",
        "# Fitting the model for TFIDF features\n",
        "lr_tfidf = lr.fit(tv_train_reviews, train_sentiments)\n",
        "print(lr_tfidf)\n",
        "\n",
        "# Predicting the model for Bag of Words\n",
        "lr_bow_predict = lr.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "# Predicting the model for TFIDF features\n",
        "lr_tfidf_predict = lr.predict(tv_test_reviews)\n",
        "print(lr_tfidf_predict)\n",
        "\n",
        "# Accuracy score for Bag of Words\n",
        "lr_bow_score = accuracy_score(test_sentiments, lr_bow_predict)\n",
        "print(\"lr_bow_score:\", lr_bow_score)\n",
        "# Accuracy score for TFIDF features\n",
        "lr_tfidf_score = accuracy_score(test_sentiments, lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score:\", lr_tfidf_score)\n",
        "\n",
        "# Classification report for Bag of Words\n",
        "lr_bow_report = classification_report(test_sentiments, lr_bow_predict, target_names=['Positive', 'Negative'])\n",
        "print(lr_bow_report)\n",
        "\n",
        "# Classification report for TFIDF features\n",
        "lr_tfidf_report = classification_report(test_sentiments, lr_tfidf_predict, target_names=['Positive', 'Negative'])\n",
        "print(lr_tfidf_report)\n",
        "\n",
        "# Confusion matrix for Bag of Words\n",
        "cm_bow = confusion_matrix(test_sentiments, lr_bow_predict, labels=[1, 0])\n",
        "print(cm_bow)\n",
        "# Confusion matrix for TFIDF features\n",
        "cm_tfidf = confusion_matrix(test_sentiments, lr_tfidf_predict, labels=[1, 0])\n",
        "print(cm_tfidf)\n",
        "\n",
        "# Training the linear SVM\n",
        "svm = SGDClassifier(loss='hinge', max_iter=500, random_state=42)\n",
        "# Fitting the SVM for Bag of Words\n",
        "svm_bow = svm.fit(cv_train_reviews, train_sentiments)\n",
        "print(svm_bow)\n",
        "# Fitting the SVM for TFIDF features\n",
        "svm_tfidf = svm.fit(tv_train_reviews, train_sentiments)\n",
        "print(svm_tfidf)\n",
        "\n",
        "# Predicting the model for Bag of Words\n",
        "svm_bow_predict = svm.predict(cv_test_reviews)\n",
        "print(svm_bow_predict)\n",
        "# Predicting the model for TFIDF features\n",
        "svm_tfidf_predict = svm.predict(tv_test_reviews)\n",
        "print(svm_tfidf_predict)\n",
        "\n",
        "# Accuracy score for Bag of Words\n",
        "svm_bow_score = accuracy_score(test_sentiments, svm_bow_predict)\n",
        "print(\"svm_bow_score:\", svm_bow_score)\n",
        "# Accuracy score for TFIDF features\n",
        "svm_tfidf_score = accuracy_score(test_sentiments, svm_tfidf_predict)\n",
        "print(\"svm_tfidf_score:\", svm_tfidf_score)\n",
        "\n",
        "# Classification report for Bag of Words\n",
        "svm_bow_report = classification_report(test_sentiments, svm_bow_predict, target_names=['Positive', 'Negative'])\n",
        "print(svm_bow_report)\n",
        "# Classification report for TFIDF features\n",
        "svm_tfidf_report = classification_report(test_sentiments, svm_tfidf_predict, target_names=['Positive', 'Negative'])\n",
        "print(svm_tfidf_report)\n",
        "\n",
        "# Confusion matrix for Bag of Words\n",
        "cm_bow = confusion_matrix(test_sentiments, svm_bow_predict, labels=[1, 0])\n",
        "print(cm_bow)\n",
        "# Confusion matrix for TFIDF features\n",
        "cm_tfidf = confusion_matrix(test_sentiments, svm_tfidf_predict, labels=[1, 0])\n",
        "print(cm_tfidf)\n",
        "\n",
        "# Training the Multinomial Naive Bayes\n",
        "mnb = MultinomialNB()\n",
        "# Fitting the model for Bag of Words\n",
        "mnb_bow = mnb.fit(cv_train_reviews, train_sentiments)\n",
        "print(mnb_bow)\n",
        "# Fitting the model for TFIDF features\n",
        "mnb_tfidf = mnb.fit(tv_train_reviews, train_sentiments)\n",
        "print(mnb_tfidf)\n",
        "\n",
        "# Predicting the model for Bag of Words\n",
        "mnb_bow_predict = mnb.predict(cv_test_reviews)\n",
        "print(mnb_bow_predict)\n",
        "# Predicting the model for TFIDF features\n",
        "mnb_tfidf_predict = mnb.predict(tv_test_reviews)\n",
        "print(mnb_tfidf_predict)\n",
        "\n",
        "# Accuracy score for Bag of Words\n",
        "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
        "print(\"mnb_bow_score:\", mnb_bow_score)\n",
        "# Accuracy score for TFIDF features\n",
        "mnb_tfidf_score = accuracy_score(test_sentiments, mnb_tfidf_predict)\n",
        "print(\"mnb_tfidf_predict:\", mnb_tfidf_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG8Fa6GDewfn",
        "outputId": "96d3cfe1-6068-4c67-bf31-d1be9ab7f567"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "5  Probably my all-time favorite movie, a story o...  positive\n",
            "6  I sure would like to see a resurrection of a u...  positive\n",
            "7  This show was an amazing, fresh & innovative i...  negative\n",
            "8  Encouraged by the positive comments about this...  negative\n",
            "9  If you like original gut wrenching laughter yo...  positive\n",
            "                                                   review sentiment\n",
            "count                                               50000     50000\n",
            "unique                                              49582         2\n",
            "top     Loved today's show!!! It was a variety and not...  positive\n",
            "freq                                                    5     25000\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "(40000,) (40000,)\n",
            "(10000,) (10000,)\n",
            "BOW_cv_train: (40000, 6208633)\n",
            "BOW_cv_test: (10000, 6208633)\n",
            "Tfidf_train: (40000, 6208633)\n",
            "Tfidf_test: (10000, 6208633)\n",
            "(50000, 1)\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n",
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n",
            "[0 0 0 ... 0 1 1]\n",
            "[0 0 0 ... 0 1 1]\n",
            "lr_bow_score: 0.7514\n",
            "lr_tfidf_score: 0.7498\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.75      0.75      0.75      4993\n",
            "    Negative       0.75      0.75      0.75      5007\n",
            "\n",
            "    accuracy                           0.75     10000\n",
            "   macro avg       0.75      0.75      0.75     10000\n",
            "weighted avg       0.75      0.75      0.75     10000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.74      0.77      0.75      4993\n",
            "    Negative       0.76      0.73      0.75      5007\n",
            "\n",
            "    accuracy                           0.75     10000\n",
            "   macro avg       0.75      0.75      0.75     10000\n",
            "weighted avg       0.75      0.75      0.75     10000\n",
            "\n",
            "[[3770 1237]\n",
            " [1249 3744]]\n",
            "[[3662 1345]\n",
            " [1157 3836]]\n",
            "SGDClassifier(max_iter=500, random_state=42)\n",
            "SGDClassifier(max_iter=500, random_state=42)\n",
            "[1 1 0 ... 1 1 1]\n",
            "[1 1 1 ... 1 1 1]\n",
            "svm_bow_score: 0.583\n",
            "svm_tfidf_score: 0.5112\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.94      0.18      0.30      4993\n",
            "    Negative       0.55      0.99      0.70      5007\n",
            "\n",
            "    accuracy                           0.58     10000\n",
            "   macro avg       0.74      0.58      0.50     10000\n",
            "weighted avg       0.74      0.58      0.50     10000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       1.00      0.02      0.04      4993\n",
            "    Negative       0.51      1.00      0.67      5007\n",
            "\n",
            "    accuracy                           0.51     10000\n",
            "   macro avg       0.75      0.51      0.36     10000\n",
            "weighted avg       0.75      0.51      0.36     10000\n",
            "\n",
            "[[4947   60]\n",
            " [4110  883]]\n",
            "[[5007    0]\n",
            " [4888  105]]\n",
            "MultinomialNB()\n",
            "MultinomialNB()\n",
            "[0 0 0 ... 0 1 1]\n",
            "[0 0 0 ... 0 1 1]\n",
            "mnb_bow_score: 0.751\n",
            "mnb_tfidf_predict: [0 0 0 ... 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSTT-J5zkvdB",
        "outputId": "7afdb7b1-81fd-4ee8-e11f-d4d2583d906c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7NBrT0vYewdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-jaUwW9ewav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAxcE5ykewLX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}